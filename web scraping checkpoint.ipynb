{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9f9ef-4778-4085-8e46-e0f5ae9df3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to Get and parse html content from a Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47bee20-5c34-4314-bb42-8464b6424b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter Talbot - Wikipedia\n",
      "Peter Talbot may refer to:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_and_parse_wikipedia_page(url):\n",
    "    try:\n",
    "        # Get the HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        return soup\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Peter_Talbot\"\n",
    "    soup = get_and_parse_wikipedia_page(url)\n",
    "\n",
    "    if soup:\n",
    "        # Example: Print the page title\n",
    "        print(soup.title.string)\n",
    "\n",
    "        # Example: Extract the first paragraph\n",
    "        first_paragraph = soup.find('p')\n",
    "        print(first_paragraph.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fd556-d206-4e00-9da2-be99874be0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to Extract article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9630ac7b-c8e4-4e3b-817d-5d7c4f354155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title: Peter Talbot\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_wikipedia_title(url):\n",
    "    try:\n",
    "        # Get the HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the title of the article\n",
    "        title = soup.find('h1', {'id': 'firstHeading'}).get_text()\n",
    "\n",
    "        return title\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"An error occurred while parsing the title: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Peter_Talbot\"\n",
    "    title = extract_wikipedia_title(url)\n",
    "\n",
    "    if title:\n",
    "        print(f\"Article Title: {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae29605-6b14-48b6-adaf-b677e05abbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to Extract article text for each paragraph with their respective headings\n",
    "# Map those headings to their respective paragraphs in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "544fb0b2-c0d2-455d-bf03-2a449e93e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading: Introduction\n",
      "\n",
      "Content: Peter Talbot may refer to:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_content(url):\n",
    "    try:\n",
    "        # Get the HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all headings and paragraphs\n",
    "        content = {}\n",
    "        current_heading = \"Introduction\"\n",
    "        content[current_heading] = []\n",
    "\n",
    "        for tag in soup.find_all(['h2', 'h3', 'p']):\n",
    "            if tag.name == 'h2' or tag.name == 'h3':\n",
    "                current_heading = tag.get_text().strip()\n",
    "                content[current_heading] = []\n",
    "            elif tag.name == 'p':\n",
    "                if current_heading not in content:\n",
    "                    content[current_heading] = []\n",
    "                content[current_heading].append(tag.get_text().strip())\n",
    "\n",
    "        # Merge paragraphs under each heading\n",
    "        for heading in content:\n",
    "            content[heading] = ' '.join(content[heading])\n",
    "\n",
    "        return content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"An error occurred while parsing the content: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Peter_Talbot\"\n",
    "    content = extract_article_content(url)\n",
    "\n",
    "    if content:\n",
    "        for heading, paragraphs in content.items():\n",
    "            print(f\"Heading: {heading}\\n\")\n",
    "            print(f\"Content: {paragraphs}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115651d8-4892-4c96-bbb3-3c02e9ecc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to collect every link that redirects to another Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63972b79-d455-4c85-a78c-4d2890c39f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "https://en.wikipedia.org/wiki/Web_scraping\n",
      "https://en.wikipedia.org/wiki/Web_scraping\n",
      "https://en.wikipedia.org/wiki/Web_scraping\n",
      "https://en.wikipedia.org/wiki/Data_scraping\n",
      "https://en.wikipedia.org/wiki/Scraper_site\n",
      "https://en.wikipedia.org/wiki/Data_scraping\n",
      "https://en.wikipedia.org/wiki/Data_extraction\n",
      "https://en.wikipedia.org/wiki/Website\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web\n",
      "https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol\n",
      "https://en.wikipedia.org/wiki/Internet_bot\n",
      "https://en.wikipedia.org/wiki/Web_crawler\n",
      "https://en.wikipedia.org/wiki/Database\n",
      "https://en.wikipedia.org/wiki/Data_retrieval\n",
      "https://en.wikipedia.org/wiki/Data_analysis\n",
      "https://en.wikipedia.org/wiki/Parsing\n",
      "https://en.wikipedia.org/wiki/Contact_scraping\n",
      "https://en.wikipedia.org/wiki/Web_indexing\n",
      "https://en.wikipedia.org/wiki/Web_mining\n",
      "https://en.wikipedia.org/wiki/Data_mining\n",
      "https://en.wikipedia.org/wiki/Comparison_shopping_website\n",
      "https://en.wikipedia.org/wiki/Change_detection_and_notification\n",
      "https://en.wikipedia.org/wiki/Web_mashup\n",
      "https://en.wikipedia.org/wiki/Web_data_integration\n",
      "https://en.wikipedia.org/wiki/Web_page\n",
      "https://en.wikipedia.org/wiki/HTML\n",
      "https://en.wikipedia.org/wiki/XHTML\n",
      "https://en.wikipedia.org/wiki/End-user_(computer_science)\n",
      "https://en.wikipedia.org/wiki/Market_research\n",
      "https://en.wikipedia.org/wiki/JSON\n",
      "https://en.wikipedia.org/wiki/Document_Object_Model\n",
      "https://en.wikipedia.org/wiki/Computer_vision\n",
      "https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web_Wanderer\n",
      "https://en.wikipedia.org/wiki/JumpStation\n",
      "https://en.wikipedia.org/wiki/Application_programming_interface\n",
      "https://en.wikipedia.org/wiki/Salesforce.com\n",
      "https://en.wikipedia.org/wiki/EBay\n",
      "https://en.wikipedia.org/wiki/Semantic_web\n",
      "https://en.wikipedia.org/wiki/Human-computer_interaction\n",
      "https://en.wikipedia.org/wiki/Grep\n",
      "https://en.wikipedia.org/wiki/Regular_expression\n",
      "https://en.wikipedia.org/wiki/Perl\n",
      "https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "https://en.wikipedia.org/wiki/Static_web_page\n",
      "https://en.wikipedia.org/wiki/Dynamic_web_page\n",
      "https://en.wikipedia.org/wiki/Socket_programming\n",
      "https://en.wikipedia.org/wiki/Wrapper_(data_mining)\n",
      "https://en.wikipedia.org/wiki/Semi-structured_data\n",
      "https://en.wikipedia.org/wiki/XQuery\n",
      "https://en.wikipedia.org/wiki/Document_Object_Model\n",
      "https://en.wikipedia.org/wiki/Internet_Explorer\n",
      "https://en.wikipedia.org/wiki/Mozilla\n",
      "https://en.wikipedia.org/wiki/XPath\n",
      "https://en.wikipedia.org/wiki/Long_Tail\n",
      "https://en.wikipedia.org/wiki/Metadata\n",
      "https://en.wikipedia.org/wiki/Microformat\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "https://en.wikipedia.org/wiki/Computer_vision\n",
      "https://en.wikipedia.org/wiki/Terms_of_service\n",
      "https://en.wikipedia.org/wiki/Cause_of_action\n",
      "https://en.wikipedia.org/wiki/Computer_Fraud_and_Abuse_Act\n",
      "https://en.wikipedia.org/wiki/Trespass_to_chattels\n",
      "https://en.wikipedia.org/wiki/Feist_Publications,_Inc.,_v._Rural_Telephone_Service_Co.\n",
      "https://en.wikipedia.org/wiki/Trespass_to_chattels\n",
      "https://en.wikipedia.org/wiki/EBay_v._Bidder%27s_Edge\n",
      "https://en.wikipedia.org/wiki/Auction_sniping\n",
      "https://en.wikipedia.org/wiki/Personal_property\n",
      "https://en.wikipedia.org/wiki/Plaintiff\n",
      "https://en.wikipedia.org/wiki/Defendant\n",
      "https://en.wikipedia.org/wiki/Screen_scraping\n",
      "https://en.wikipedia.org/wiki/American_Airlines\n",
      "https://en.wikipedia.org/wiki/Injunction\n",
      "https://en.wikipedia.org/wiki/Southwest_Airlines\n",
      "https://en.wikipedia.org/wiki/US_Copyright_law\n",
      "https://en.wikipedia.org/wiki/Supreme_Court_of_the_United_States\n",
      "https://en.wikipedia.org/wiki/Yahoo!\n",
      "https://en.wikipedia.org/wiki/Craigslist_v._3Taps\n",
      "https://en.wikipedia.org/wiki/Computer_Fraud_and_Abuse_Act\n",
      "https://en.wikipedia.org/wiki/Cvent,_Inc.\n",
      "https://en.wikipedia.org/wiki/Eventbrite\n",
      "https://en.wikipedia.org/wiki/Browse_wrap\n",
      "https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Eastern_District_of_Pennsylvania\n",
      "https://en.wikipedia.org/wiki/QVC\n",
      "https://en.wikipedia.org/wiki/Facebook,_Inc._v._Power_Ventures,_Inc.\n",
      "https://en.wikipedia.org/wiki/Electronic_Frontier_Foundation\n",
      "https://en.wikipedia.org/wiki/Associated_Press_v._Meltwater_U.S._Holdings,_Inc.\n",
      "https://en.wikipedia.org/wiki/Ninth_Circuit\n",
      "https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn\n",
      "https://en.wikipedia.org/wiki/United_States_Supreme_Court\n",
      "https://en.wikipedia.org/wiki/Van_Buren_v._United_States\n",
      "https://en.wikipedia.org/wiki/Internet_Archive\n",
      "https://en.wikipedia.org/wiki/Maritime_and_Commercial_Court_(Denmark)\n",
      "https://en.wikipedia.org/wiki/Inchoate_offense\n",
      "https://en.wikipedia.org/wiki/Ryanair\n",
      "https://en.wikipedia.org/wiki/Clickwrap\n",
      "https://en.wikipedia.org/wiki/Michael_Hanna_(judge)\n",
      "https://en.wikipedia.org/wiki/Spam_Act_2003\n",
      "https://en.wikipedia.org/wiki/IP_address\n",
      "https://en.wikipedia.org/wiki/Geolocation\n",
      "https://en.wikipedia.org/wiki/DNSBL\n",
      "https://en.wikipedia.org/wiki/Web_service\n",
      "https://en.wikipedia.org/wiki/Application_programming_interface\n",
      "https://en.wikipedia.org/wiki/User_agent\n",
      "https://en.wikipedia.org/wiki/String_(computer_science)\n",
      "https://en.wikipedia.org/wiki/Robots_exclusion_standard\n",
      "https://en.wikipedia.org/wiki/Googlebot\n",
      "https://en.wikipedia.org/wiki/CAPTCHA\n",
      "https://en.wikipedia.org/wiki/Application_firewall\n",
      "https://en.wikipedia.org/wiki/Honeypot_(computing)\n",
      "https://en.wikipedia.org/wiki/Obfuscation\n",
      "https://en.wikipedia.org/wiki/CSS_sprite\n",
      "https://en.wikipedia.org/wiki/Web_accessibility\n",
      "https://en.wikipedia.org/wiki/Screen_reader\n",
      "https://en.wikipedia.org/wiki/Robots_exclusion_standard\n",
      "https://en.wikipedia.org/wiki/Archive.today\n",
      "https://en.wikipedia.org/wiki/Comparison_of_feed_aggregators\n",
      "https://en.wikipedia.org/wiki/Data_scraping\n",
      "https://en.wikipedia.org/wiki/Data_wrangling\n",
      "https://en.wikipedia.org/wiki/Importer_(computing)\n",
      "https://en.wikipedia.org/wiki/Job_wrapping\n",
      "https://en.wikipedia.org/wiki/Knowledge_extraction\n",
      "https://en.wikipedia.org/wiki/OpenSocial\n",
      "https://en.wikipedia.org/wiki/Scraper_site\n",
      "https://en.wikipedia.org/wiki/Fake_news_website\n",
      "https://en.wikipedia.org/wiki/Blog_scraping\n",
      "https://en.wikipedia.org/wiki/Spamdexing\n",
      "https://en.wikipedia.org/wiki/Domain_name_drop_list\n",
      "https://en.wikipedia.org/wiki/Text_corpus\n",
      "https://en.wikipedia.org/wiki/Web_archiving\n",
      "https://en.wikipedia.org/wiki/Web_crawler\n",
      "https://en.wikipedia.org/wiki/Offline_reader\n",
      "https://en.wikipedia.org/wiki/Link_farm\n",
      "https://en.wikipedia.org/wiki/Search_engine_scraping\n",
      "https://en.wikipedia.org/wiki/Doi_(identifier)\n",
      "https://en.wikipedia.org/wiki/ISSN_(identifier)\n",
      "https://en.wikipedia.org/wiki/S2CID_(identifier)\n",
      "https://en.wikipedia.org/wiki/Doi_(identifier)\n",
      "https://en.wikipedia.org/wiki/ISBN_(identifier)\n",
      "https://en.wikipedia.org/wiki/S2CID_(identifier)\n",
      "https://en.wikipedia.org/wiki/Doi_(identifier)\n",
      "https://en.wikipedia.org/wiki/ISSN_(identifier)\n",
      "https://en.wikipedia.org/wiki/Doi_(identifier)\n",
      "https://en.wikipedia.org/wiki/ISSN_(identifier)\n",
      "https://en.wikipedia.org/wiki/Reuters\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def collect_wikipedia_links(url):\n",
    "    try:\n",
    "        # Get the HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all links that redirect to other Wikipedia pages\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            if href.startswith('/wiki/') and not ':' in href:\n",
    "                full_url = f\"https://en.wikipedia.org{href}\"\n",
    "                links.append(full_url)\n",
    "\n",
    "        return links\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "    links = collect_wikipedia_links(url)\n",
    "\n",
    "    if links:\n",
    "        for link in links:\n",
    "            print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3e8f3-7050-4404-bc4e-b619c279f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all the previous functions into a single function that takes as parameters a Wikipedia link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "059994bf-c5ff-4d56-a68a-c54416e83428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Peter Talbot\n",
      "\n",
      "Content:\n",
      "Heading: Introduction\n",
      "\n",
      "Content: Peter Talbot may refer to:\n",
      "\n",
      "Links:\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "https://en.wikipedia.org/wiki/Peter_Talbot\n",
      "https://en.wikipedia.org/wiki/Peter_Talbot\n",
      "https://en.wikipedia.org/wiki/Peter_Talbot\n",
      "https://en.wikipedia.org/wiki/Peter_Talbot_(bishop)\n",
      "https://en.wikipedia.org/wiki/Peter_Talbot_(politician)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_wikipedia_data(url):\n",
    "    try:\n",
    "        # Get the HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Function to extract the title\n",
    "        def extract_title(soup):\n",
    "            try:\n",
    "                title = soup.find('h1', {'id': 'firstHeading'}).get_text()\n",
    "                return title\n",
    "            except AttributeError as e:\n",
    "                print(f\"An error occurred while parsing the title: {e}\")\n",
    "                return None\n",
    "\n",
    "        # Function to extract article content\n",
    "        def extract_content(soup):\n",
    "            content = {}\n",
    "            current_heading = \"Introduction\"\n",
    "            content[current_heading] = []\n",
    "\n",
    "            for tag in soup.find_all(['h2', 'h3', 'p']):\n",
    "                if tag.name == 'h2' or tag.name == 'h3':\n",
    "                    current_heading = tag.get_text().strip()\n",
    "                    content[current_heading] = []\n",
    "                elif tag.name == 'p':\n",
    "                    if current_heading not in content:\n",
    "                        content[current_heading] = []\n",
    "                    content[current_heading].append(tag.get_text().strip())\n",
    "\n",
    "            # Merge paragraphs under each heading\n",
    "            for heading in content:\n",
    "                content[heading] = ' '.join(content[heading])\n",
    "\n",
    "            return content\n",
    "\n",
    "        # Function to collect Wikipedia links\n",
    "        def collect_links(soup):\n",
    "            links = []\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/wiki/') and not ':' in href:\n",
    "                    full_url = f\"https://en.wikipedia.org{href}\"\n",
    "                    links.append(full_url)\n",
    "            return links\n",
    "\n",
    "        # Extract data\n",
    "        title = extract_title(soup)\n",
    "        content = extract_content(soup)\n",
    "        links = collect_links(soup)\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"links\": links\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Peter_Talbot\"\n",
    "    data = extract_wikipedia_data(url)\n",
    "\n",
    "    if data:\n",
    "        print(f\"Title: {data['title']}\\n\")\n",
    "        \n",
    "        print(\"Content:\")\n",
    "        for heading, paragraphs in data['content'].items():\n",
    "            print(f\"Heading: {heading}\\n\")\n",
    "            print(f\"Content: {paragraphs}\\n\")\n",
    "        \n",
    "        print(\"Links:\")\n",
    "        for link in data['links']:\n",
    "            print(link)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
